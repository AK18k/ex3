{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AK18k/ex3/blob/main/Ex3_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AK18k/ex3\n"
      ],
      "metadata": {
        "id": "_gmAF3KS3EDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04df342e-cd66-4eba-8000-3dce63d1ca98"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ex3'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 19 (delta 6), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (19/19), 29.45 MiB | 7.71 MiB/s, done.\n",
            "Updating files: 100% (7/7), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torchvision"
      ],
      "metadata": {
        "id": "BUPkphR6CVlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_PATH = '/content/drive/MyDrive/ex3/data'\n",
        "PATH = '/content/drive/MyDrive/ex3'\n",
        "#os.chdir('/content/drive/MyDrive/ex3')\n",
        "#!ls"
      ],
      "metadata": {
        "id": "3qiSqOGVcGFy",
        "outputId": "b551133c-ecd0-4cd4-b400-595e4f003865",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.modules.activation import Softplus\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# Define hyperparameters\n",
        "input_size = 28 * 28  # Size of the input images (28x28 pixels)\n",
        "latent_size = 50  # Length of the latent vector\n",
        "VAE_batch_size = 64\n",
        "SVM_batch_size = 64\n",
        "VAE_epochs = 10\n",
        "SVM_epochs = 10\n",
        "learning_rate = 1e-3\n",
        "num_hidden_units = 600\n",
        "num_of_labeled_samples = 100\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "g3oK_4RagXtQ",
        "outputId": "ee0bae7a-680c-476b-dc13-f064ffa796f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8644226250>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load FashionMNIST dataset\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_dataset = datasets.FashionMNIST(root='data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.FashionMNIST(root=DATA_PATH, train=False, transform=transform, download=True)"
      ],
      "metadata": {
        "id": "w0CKdNquiISo",
        "outputId": "a16d0ed0-81e0-424e-f3a8-7a2e6d374d70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 12500869.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 212373.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3907536.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 5891480.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################\n",
        "# Split a dataset to labled and unlabled datasets\n",
        "# Inputs:\n",
        "#   - the dataset\n",
        "#   - number of labled samples required from the dataset\n",
        "# Output:\n",
        "#   - labled and unlabled datasets (not dataloaders)\n",
        "##############################################################\n",
        "\n",
        "# Define custom dataset for labeled and unlabeled data\n",
        "class SplitDataset(Dataset):\n",
        "    def __init__(self, dataset, labeled_indices):\n",
        "        self.dataset = dataset\n",
        "        self.labeled_indices = labeled_indices\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.dataset[self.labeled_indices[index]]\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labeled_indices)\n",
        "\n",
        "\n",
        "def split_to_labeled(train_dataset, num_of_labeled_samples):\n",
        "  # Determine the number of labeled samples per class\n",
        "  num_classes = len(train_dataset.classes)\n",
        "  labeled_samples_per_class = num_of_labeled_samples // num_classes  # N is the desired number of labeled samples\n",
        "\n",
        "  # Split the dataset into labeled and unlabeled data\n",
        "  labeled_indices = []\n",
        "  unlabeled_indices = []\n",
        "  class_counts = [0] * num_classes\n",
        "\n",
        "  for i, (image, label) in enumerate(train_dataset):\n",
        "      if class_counts[label] < labeled_samples_per_class:\n",
        "          labeled_indices.append(i)\n",
        "          class_counts[label] += 1\n",
        "      else:\n",
        "          unlabeled_indices.append(i)\n",
        "\n",
        "  # Create labeled and unlabeled datasets\n",
        "  labeled_dataset = SplitDataset(train_dataset, labeled_indices)\n",
        "  unlabeled_dataset = torch.utils.data.Subset(train_dataset, unlabeled_indices)\n",
        "\n",
        "  # Print the number of labeled and unlabeled samples\n",
        "  print('Dataset split:')\n",
        "  print('--------------')\n",
        "  print(f\"Number of labeled samples: {len(labeled_dataset)}\")\n",
        "  print(f\"Number of unlabeled samples: {len(unlabeled_dataset)}\")\n",
        "\n",
        "  return labeled_dataset, unlabeled_dataset\n",
        "  '''\n",
        "  # Example usage: Creating data loaders\n",
        "  labeled_batch_size = 64\n",
        "  unlabeled_batch_size = 128\n",
        "\n",
        "  labeled_loader = DataLoader(labeled_dataset, batch_size=labeled_batch_size, shuffle=True)\n",
        "  unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=unlabeled_batch_size, shuffle=True)\n",
        "\n",
        "  '''\n",
        "\n",
        "  #def expand_labled_dataset(labeled_dataset, expand_factor):\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "45FlveOimQHC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################################\n",
        "# Create and train the VAE model\n",
        "# Input:\n",
        "#   - data_loader - a dataloader with images and labels\n",
        "# Output:\n",
        "#   - the VAE model\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "# Define the VAE architecture\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_size, latent_size):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, num_hidden_units),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(num_hidden_units, latent_size * 2)  # Output mu and logvar for each latent dimension\n",
        "        )\n",
        "\n",
        "        # Decoder layers\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_size, num_hidden_units),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(num_hidden_units, input_size),\n",
        "            nn.Sigmoid()  # Output values between 0 and 1\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        return z\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        encoded = self.encoder(x)\n",
        "        mu, logvar = torch.chunk(encoded, 2, dim=1)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # Decoder\n",
        "        reconstructed = self.decoder(z)\n",
        "        return reconstructed, mu, logvar\n",
        "\n",
        "\n",
        "def train_VAE(data_loader):\n",
        "  # Create VAE model\n",
        "  VAE_model = VAE(input_size, latent_size).to(device)\n",
        "\n",
        "  # Define loss function\n",
        "  criterion = nn.BCELoss(reduction='sum')  # Binary cross-entropy loss\n",
        "\n",
        "  # Define optimizer\n",
        "  optimizer = optim.Adam(VAE_model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(VAE_epochs):\n",
        "      for i, (images, _) in enumerate(data_loader):\n",
        "          # Flatten input images\n",
        "          images = images.view(images.size(0), -1).to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          reconstructed, mu, logvar = VAE_model(images)\n",
        "\n",
        "          # Compute reconstruction loss and KL divergence\n",
        "          reconstruction_loss = criterion(reconstructed, images)\n",
        "          kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "          # Total loss\n",
        "          loss = reconstruction_loss + kl_divergence\n",
        "\n",
        "          # Backward and optimize\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 100 == 0:\n",
        "              print(f\"VAE train Epoch [{epoch+1}/{VAE_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "  torch.save(VAE_model.state_dict(), 'VAE_model.pth')\n",
        "  print('model saved')\n",
        "\n",
        "  return VAE_model\n",
        "\n"
      ],
      "metadata": {
        "id": "aokcicqztgDQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=VAE_batch_size, shuffle=True)\n",
        "VAE_epochs = 3\n",
        "VAE_model = train_VAE(train_loader)"
      ],
      "metadata": {
        "id": "w2Wj1lmP5Zuu",
        "outputId": "084b3149-095f-4eb4-8d96-0cfc0b777367",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAE train Epoch [1/3], Step [100/938], Loss: 18529.2227\n",
            "VAE train Epoch [1/3], Step [200/938], Loss: 18389.8281\n",
            "VAE train Epoch [1/3], Step [300/938], Loss: 17878.8555\n",
            "VAE train Epoch [1/3], Step [400/938], Loss: 17330.6562\n",
            "VAE train Epoch [1/3], Step [500/938], Loss: 17500.9160\n",
            "VAE train Epoch [1/3], Step [600/938], Loss: 16269.8086\n",
            "VAE train Epoch [1/3], Step [700/938], Loss: 17263.5078\n",
            "VAE train Epoch [1/3], Step [800/938], Loss: 17713.7402\n",
            "VAE train Epoch [1/3], Step [900/938], Loss: 15500.0059\n",
            "VAE train Epoch [2/3], Step [100/938], Loss: 16879.0293\n",
            "VAE train Epoch [2/3], Step [200/938], Loss: 15902.8633\n",
            "VAE train Epoch [2/3], Step [300/938], Loss: 15325.1240\n",
            "VAE train Epoch [2/3], Step [400/938], Loss: 15714.0469\n",
            "VAE train Epoch [2/3], Step [500/938], Loss: 16077.1377\n",
            "VAE train Epoch [2/3], Step [600/938], Loss: 16031.1338\n",
            "VAE train Epoch [2/3], Step [700/938], Loss: 16280.8359\n",
            "VAE train Epoch [2/3], Step [800/938], Loss: 16240.6865\n",
            "VAE train Epoch [2/3], Step [900/938], Loss: 15595.8096\n",
            "VAE train Epoch [3/3], Step [100/938], Loss: 15476.8848\n",
            "VAE train Epoch [3/3], Step [200/938], Loss: 16690.1152\n",
            "VAE train Epoch [3/3], Step [300/938], Loss: 15907.3564\n",
            "VAE train Epoch [3/3], Step [400/938], Loss: 16076.4238\n",
            "VAE train Epoch [3/3], Step [500/938], Loss: 16516.8066\n",
            "VAE train Epoch [3/3], Step [600/938], Loss: 16147.4062\n",
            "VAE train Epoch [3/3], Step [700/938], Loss: 15640.5977\n",
            "VAE train Epoch [3/3], Step [800/938], Loss: 15977.1396\n",
            "VAE train Epoch [3/3], Step [900/938], Loss: 15113.4980\n",
            "model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################\n",
        "# Passes a dataset of images through a pretrained VAE model\n",
        "# Inputs:\n",
        "#   - VAE_model\n",
        "#   - data - the dataset (not a dataloader)\n",
        "# Output:\n",
        "#   - output_vectors - latent vectors\n",
        "###############################################################\n",
        "\n",
        "def images_to_latent(VAE_model, data):\n",
        "  data_loader = DataLoader(data, shuffle=False) # Ofer removed batch_size=VAE_batch_size because there is no training\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "  VAE_model.eval()\n",
        "\n",
        "  # Create an empty list to store the output vectors\n",
        "  z_output_vectors = []\n",
        "  mu_output_vectors = []\n",
        "  logvar_output_vectors = []\n",
        "\n",
        "  # Pass the dataset through the VAE model\n",
        "  with torch.no_grad():\n",
        "      for images, _ in data_loader:\n",
        "          images = images.to(device)\n",
        "          # Obtain the output vectors from the VAE model\n",
        "          z, mu, logvar = VAE_model(images)\n",
        "          z_output_vectors.append(z)\n",
        "          mu_output_vectors.append(mu)\n",
        "          logvar_output_vectors.append(logvar)\n",
        "\n",
        "  # Concatenate the output vectors into a single tensor\n",
        "  output_vectors = torch.cat(output_vectors, dim=0)\n",
        "\n",
        "  return z_output_vectors, mu_output_vectors, logvar_output_vectors"
      ],
      "metadata": {
        "id": "tp_TWV9lrjQm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z, mu, logvar = images_to_latent(VAE_model, train_dataset)"
      ],
      "metadata": {
        "id": "cfQvO0n6wsSH",
        "outputId": "e0a3f52a-ebaa-4c46-ded1-08b1c20b6bf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c89dd9f65c73>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages_to_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAE_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-afe84e4c3b78>\u001b[0m in \u001b[0;36mimages_to_latent\u001b[0;34m(VAE_model, data)\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m           \u001b[0;31m# Obtain the output vectors from the VAE model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m           \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m           \u001b[0mz_output_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0mmu_output_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5336017fbe08>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x600)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################\n",
        "# Create, train and test linear SVM model (pytorch)\n",
        "# train_linSVM: trains the SVM model.\n",
        "#           if TSVM is required - input a split dataset using split_to_labled(train_dataset, num_of_labeled_samples)\n",
        "# Input:\n",
        "#  - train_dataset (not dataloader)\n",
        "# Output:\n",
        "#  - linSVM_model\n",
        "#\n",
        "# eval_linSVM: evaluates on test dataset\n",
        "# - Input: linSVM_model, test_dataset (not dataloader)\n",
        "# - Output: accuracy\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the SVM model\n",
        "class linSVM(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SVM, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, 10)  # 10 classes for FashionMNIST\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "def train_linSVM(train_dataset):\n",
        "\n",
        "  # Prepare dataloader\n",
        "  Y_train = train_dataset.targets\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=SVM_batch_size, shuffle=True)\n",
        "\n",
        "  # Create the SVM model\n",
        "  linSVM_model = linSVM(latent_size).to(device)\n",
        "  linSVM_model.train()\n",
        "\n",
        "  # Define the loss function\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Define the optimizer\n",
        "  optimizer = optim.SGD(SVM_model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(SVM_epochs):\n",
        "      for i, (images, _) in enumerate(train_loader):\n",
        "          # Flatten input images\n",
        "          images = images.view(images.size(0), -1).to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = linSVM_model(images)\n",
        "          loss = criterion(outputs, Y_train[SVM_batch_size*i:SVM_batch_size*(i+1)].to(device))\n",
        "\n",
        "          # Backward and optimize\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Print training progress\n",
        "          if (i+1) % 100 == 0:\n",
        "              print(f\"linSVM Train Epoch [{epoch+1}/{SVM_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "  return linSVM_model\n",
        "\n",
        "def eval_linSVM(linSVM_model, test_dataset):\n",
        "\n",
        "  Y_test = test_dataset.targets\n",
        "  test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=SVM_batch_size, shuffle=False)\n",
        "\n",
        "  linSVM_model.eval()\n",
        "  with torch.no_grad():\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for images, labels in test_loader:\n",
        "          images = images.view(images.size(0), -1).to(device)\n",
        "          outputs = linSVM_model(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted.cpu() == labels).sum().item()\n",
        "\n",
        "      accuracy = correct / total\n",
        "      print(f\"linSVM Test Accuracy: {accuracy:.4f}\"\n",
        "  return accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "UuRHobL3dtHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def SVM_model(kernel='poly', degree-3):\n",
        "  SVM_model = SVC(kernel=kernel, degree=degree)\n",
        "  return SVM_model\n",
        "\n",
        "def train_SVM(SVM_model, train_dataset)\n",
        "  SVM_model.fit(train_dataset.data, train_dataset.targets)\n",
        "\n",
        "  return SVM_model\n",
        "\n",
        "def test_SVM(SVM_model, test_dataset)\n",
        "\n",
        "  accuracy = SVM_model.score(test_dataset.data, test_dataset.targets)\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EMUm_73u7pRW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}