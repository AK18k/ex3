{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AK18k/ex3/blob/main/Ex3_3_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AK18k/ex3\n"
      ],
      "metadata": {
        "id": "_gmAF3KS3EDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2628be1f-770a-4f51-f4a6-d9a0b1334900"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ex3'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 28 (delta 12), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), 29.46 MiB | 8.37 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "DATA_PATH = '/content/drive/ex3/data'\n",
        "PATH = '/content/drive/ex3'\n"
      ],
      "metadata": {
        "id": "3qiSqOGVcGFy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.modules.activation import Softplus\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# Define hyperparameters\n",
        "input_size = 28 * 28  # Size of the input images (28x28 pixels)\n",
        "latent_size = 50  # Length of the latent vector\n",
        "VAE_batch_size = 64\n",
        "SVM_batch_size = 64\n",
        "VAE_epochs = 3\n",
        "SVM_epochs = 10\n",
        "learning_rate = 1e-3\n",
        "num_hidden_units = 600\n",
        "num_of_labeled_samples = 100\n",
        "expansion_rate = 10\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "g3oK_4RagXtQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5e3bf8-0438-4063-9957-17b992dd4095"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7488c8dcf0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load FashionMNIST dataset\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_dataset = datasets.FashionMNIST(root='data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.FashionMNIST(root=DATA_PATH, train=False, transform=transform, download=True)"
      ],
      "metadata": {
        "id": "w0CKdNquiISo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc43349-24ef-4438-bee0-6ec097d40948"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 8230995.95it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 141058.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 2594393.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 20447232.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /content/drive/ex3/data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 8291555.38it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/ex3/data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /content/drive/ex3/data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /content/drive/ex3/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 138982.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/ex3/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /content/drive/ex3/data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /content/drive/ex3/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 2605863.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/ex3/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /content/drive/ex3/data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /content/drive/ex3/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 6724471.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/ex3/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /content/drive/ex3/data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_to_labeled(original_dataset, num_of_labeled_samples):\n",
        "  class_count = {}\n",
        "  for _, label in original_dataset:\n",
        "      if label in class_count:\n",
        "          class_count[label] += 1\n",
        "      else:\n",
        "          class_count[label] = 1\n",
        "\n",
        "  # Calculate the desired number of samples for each class in the new dataset\n",
        "  samples_per_class = num_of_labeled_samples // len(class_count)\n",
        "\n",
        "  # Create a list to store the selected samples\n",
        "  selected_samples = []\n",
        "\n",
        "  # Iterate through the original dataset and select the desired number of samples from each class\n",
        "  selected_count = {label: 0 for label in class_count.keys()}\n",
        "  for data, label in original_dataset:\n",
        "      if selected_count[label] < samples_per_class:\n",
        "          selected_samples.append((data, label))\n",
        "          selected_count[label] += 1\n",
        "\n",
        "  class NewDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.data = [data for data, _ in samples]\n",
        "        self.labels = [label for _, label in samples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.labels[index]\n",
        "\n",
        "  # Create an instance of the new dataset using the selected samples\n",
        "  labled_dataset = NewDataset(selected_samples)\n",
        "\n",
        "  return labled_dataset\n",
        "\n",
        "def reparameterize(mu, logvar):\n",
        "    std = torch.exp(0.5 * logvar)\n",
        "    eps = torch.randn_like(std)\n",
        "    z = mu + eps * std\n",
        "    return z\n"
      ],
      "metadata": {
        "id": "FMxqzWxpVu-j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################################\n",
        "# Create and train the VAE model\n",
        "# Input:\n",
        "#   - data_loader - a dataloader with images and labels\n",
        "# Output:\n",
        "#   - the VAE model\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "# Define the VAE architecture\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_size, latent_size):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, num_hidden_units),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(num_hidden_units, latent_size * 2)  # Output mu and logvar for each latent dimension\n",
        "        )\n",
        "\n",
        "        # Decoder layers\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_size, num_hidden_units),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(num_hidden_units, input_size),\n",
        "            nn.Sigmoid()  # Output values between 0 and 1\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        return z\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        encoded = self.encoder(x)\n",
        "        mu, logvar = torch.chunk(encoded, 2, dim=1)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # Decoder\n",
        "        reconstructed = self.decoder(z)\n",
        "        return reconstructed, mu, logvar\n",
        "\n",
        "\n",
        "def train_VAE(data_loader):\n",
        "  # Create VAE model\n",
        "  VAE_model = VAE(input_size, latent_size).to(device)\n",
        "\n",
        "  # Define loss function\n",
        "  criterion = nn.BCELoss(reduction='sum')  # Binary cross-entropy loss\n",
        "\n",
        "  # Define optimizer\n",
        "  optimizer = optim.Adam(VAE_model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(VAE_epochs):\n",
        "      for i, (images, _) in enumerate(data_loader):\n",
        "          # Flatten input images\n",
        "          images = images.view(images.size(0), -1).to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          reconstructed, mu, logvar = VAE_model(images)\n",
        "\n",
        "          # Compute reconstruction loss and KL divergence\n",
        "          reconstruction_loss = criterion(reconstructed, images)\n",
        "          kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "          # Total loss\n",
        "          loss = reconstruction_loss + kl_divergence\n",
        "\n",
        "          # Backward and optimize\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 100 == 0:\n",
        "              print(f\"VAE train Epoch [{epoch+1}/{VAE_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "  torch.save(VAE_model.state_dict(), 'VAE_model.pth')\n",
        "  print('model saved')\n",
        "\n",
        "  return VAE_model\n",
        "\n"
      ],
      "metadata": {
        "id": "aokcicqztgDQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################\n",
        "# Passes a dataset of images through a pretrained VAE model\n",
        "# Inputs:\n",
        "#   - VAE_model\n",
        "#   - data - the dataset (not a dataloader)\n",
        "# Output:\n",
        "#   - output_vectors - latent vectors\n",
        "###############################################################\n",
        "\n",
        "def images_to_latent(VAE_model, data):\n",
        "  #data_loader = DataLoader(data, shuffle=False) # Ofer removed batch_size=VAE_batch_size because there is no training\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "  VAE_model.eval()\n",
        "\n",
        "  # Create an empty list to store the output vectors\n",
        "  z_output_vectors = []\n",
        "  mu_output_vectors = []\n",
        "  logvar_output_vectors = []\n",
        "  z_output_vectors.to(device)\n",
        "  mu_output_vectors.to(device)\n",
        "  logvar_output_vectors.to(device)\n",
        "  # Pass the dataset through the VAE model\n",
        "  with torch.no_grad():\n",
        "      for images, _ in data_loader:\n",
        "          images = images.to(device)\n",
        "          # Obtain the output vectors from the VAE model\n",
        "          z, mu, logvar = VAE_model(data_loader)\n",
        "          z_output_vectors.append(z)\n",
        "          mu_output_vectors.append(mu)\n",
        "          logvar_output_vectors.append(logvar)\n",
        "\n",
        "  # Concatenate the output vectors into a single tensor\n",
        "  output_vectors = torch.cat(output_vectors, dim=0)\n",
        "\n",
        "  return z_output_vectors, mu_output_vectors, logvar_output_vectors"
      ],
      "metadata": {
        "id": "tp_TWV9lrjQm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def SVM_model(kernel='poly', degree=3):\n",
        "  SVM_model = SVC(kernel=kernel, degree=degree)\n",
        "  return SVM_model\n",
        "\n",
        "def train_SVM(SVM_model, train_dataset):\n",
        "  data = train_dataset.data\n",
        "  data = data.cpu()\n",
        "  data_np = data.detach().numpy()\n",
        "\n",
        "  label = train_dataset.targets\n",
        "  label = label.cpu()\n",
        "  label_np = label.detach().numpy()\n",
        "  SVM_model.fit(data_np, label_np)\n",
        "\n",
        "  return SVM_model\n",
        "\n",
        "def test_SVM(SVM_model, test_dataset):\n",
        "  data = test_dataset.data\n",
        "  data = data.cpu()\n",
        "  data_np = data.detach().numpy()\n",
        "\n",
        "  label = test_dataset.targets\n",
        "  label = label.cpu()\n",
        "  label_np = label.detach().numpy()\n",
        "  accuracy = SVM_model.score(data_np, label_np)\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EMUm_73u7pRW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reparameterize(mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        return z"
      ],
      "metadata": {
        "id": "p1KQGy5orDrb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=VAE_batch_size, shuffle=True)\n",
        "VAE_model = train_VAE(train_loader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cfQvO0n6wsSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4378cd06-ee2d-44ac-efbb-447016a1953d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAE train Epoch [1/3], Step [100/938], Loss: 18654.8789\n",
            "VAE train Epoch [1/3], Step [200/938], Loss: 19123.7988\n",
            "VAE train Epoch [1/3], Step [300/938], Loss: 17301.4824\n",
            "VAE train Epoch [1/3], Step [400/938], Loss: 17060.1699\n",
            "VAE train Epoch [1/3], Step [500/938], Loss: 18406.0723\n",
            "VAE train Epoch [1/3], Step [600/938], Loss: 16799.8047\n",
            "VAE train Epoch [1/3], Step [700/938], Loss: 16766.6582\n",
            "VAE train Epoch [1/3], Step [800/938], Loss: 16517.6582\n",
            "VAE train Epoch [1/3], Step [900/938], Loss: 16050.0830\n",
            "VAE train Epoch [2/3], Step [100/938], Loss: 15607.9961\n",
            "VAE train Epoch [2/3], Step [200/938], Loss: 16278.2148\n",
            "VAE train Epoch [2/3], Step [300/938], Loss: 16359.7520\n",
            "VAE train Epoch [2/3], Step [400/938], Loss: 16190.3535\n",
            "VAE train Epoch [2/3], Step [500/938], Loss: 15984.1348\n",
            "VAE train Epoch [2/3], Step [600/938], Loss: 14829.5420\n",
            "VAE train Epoch [2/3], Step [700/938], Loss: 15757.3506\n",
            "VAE train Epoch [2/3], Step [800/938], Loss: 15931.0830\n",
            "VAE train Epoch [2/3], Step [900/938], Loss: 16204.8340\n",
            "VAE train Epoch [3/3], Step [100/938], Loss: 15573.0205\n",
            "VAE train Epoch [3/3], Step [200/938], Loss: 16292.0488\n",
            "VAE train Epoch [3/3], Step [300/938], Loss: 17204.7598\n",
            "VAE train Epoch [3/3], Step [400/938], Loss: 16543.4707\n",
            "VAE train Epoch [3/3], Step [500/938], Loss: 16884.0820\n",
            "VAE train Epoch [3/3], Step [600/938], Loss: 16676.3086\n",
            "VAE train Epoch [3/3], Step [700/938], Loss: 16040.5840\n",
            "VAE train Epoch [3/3], Step [800/938], Loss: 15879.6387\n",
            "VAE train Epoch [3/3], Step [900/938], Loss: 15438.8867\n",
            "model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the train dataset\n",
        "print(f'Spliting the training dataset to {num_of_labeled_samples} labled samples')\n",
        "labled_dataset = split_to_labeled(train_dataset, num_of_labeled_samples)\n",
        "\n",
        "z_data = torch.zeros([0,50]).to(device)\n",
        "z_label = torch.zeros([0]).to(device)\n",
        "\n",
        "expansion_rate = 15\n",
        "\n",
        "#create the expanded latent vector space\n",
        "print('Learning mu and logvar from labeled dataset')\n",
        "data_loader = DataLoader(labled_dataset, batch_size=VAE_batch_size, shuffle=False)\n",
        "for batch in data_loader:\n",
        "    images, _ = batch  # Assuming you don't need the labels\n",
        "    print(f'batch.shape={images.size()}')\n",
        "    images = images.view(images.size(0), -1).to(device)\n",
        "    _, mu, logvar = VAE_model(images)\n",
        "\n",
        "    print(f'Expanding the labeled dataset by {expansion_rate}')\n",
        "    for i in range(0,len(mu)):\n",
        "      for e in range(0,expansion_rate):\n",
        "        new_z = reparameterize(mu[i], logvar[i])\n",
        "        new_z = (new_z.unsqueeze(0))\n",
        "        new_label = torch.tensor(labled_dataset.labels[i]).to(device)\n",
        "        new_label = (new_label.unsqueeze(0))\n",
        "        z_data = torch.cat((z_data, new_z), 0)\n",
        "        z_label = torch.cat((z_label, new_label), 0).to(torch.int)\n",
        "\n",
        "print(f'after z_data.shape = {z_data.shape}')\n",
        "print(f'after z_label.shape = {z_label.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikcreYJa-n3_",
        "outputId": "127b15d6-f640-48b8-c2d0-5e325acfcf9c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spliting the training dataset to 100 labled samples\n",
            "Learning mu and logvar from labeled dataset\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 15\n",
            "batch.shape=torch.Size([36, 1, 28, 28])\n",
            "Expanding the labeled dataset by 15\n",
            "after z_data.shape = torch.Size([1500, 50])\n",
            "after z_label.shape = torch.Size([1500])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total entries to SVM train: {z_data.size()}')\n",
        "\n",
        "SVM = SVM_model(kernel='poly', degree=3)\n",
        "\n",
        "print('Training the SVM with expanded latent dataset')\n",
        "latent_dataset = Dataset()\n",
        "latent_dataset.data = z_data\n",
        "latent_dataset.targets = z_label\n",
        "\n",
        "SVM = train_SVM(SVM, latent_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ7VTPVkwmjU",
        "outputId": "6e034628-9d2c-4e8c-f06e-06939e49420c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total entries to SVM train: torch.Size([1500, 50])\n",
            "Training the SVM with expanded latent dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "z_data = torch.zeros([0,50]).to(device)\n",
        "z_label = torch.zeros([0]).to(device)\n",
        "\n",
        "i = 0\n",
        "for batch in data_loader:\n",
        "    images, _ = batch  # Assuming you don't need the labels\n",
        "    images = images.view(images.size(0), -1).to(device)\n",
        "    _, mu, logvar = VAE_model(images)\n",
        "    #mu = m[:,0]\n",
        "    #logvar = l[:,0]\n",
        "    new_z = reparameterize(mu, logvar)\n",
        "    new_z = (new_z.squeeze(1))\n",
        "    new_label = test_dataset.targets[i].clone().detach().to(device)\n",
        "    new_label1 = new_label.unsqueeze(0)\n",
        "    z_data = torch.cat((z_data, new_z), 0)\n",
        "    z_label = torch.cat((z_label, new_label1), 0).to(torch.int)\n",
        "    i += 1\n",
        "\n",
        "result = test_SVM(SVM, latent_dataset)\n",
        "\n",
        "print(f'Final result of SVM on test dataset: {result}')\n"
      ],
      "metadata": {
        "id": "c0jAuPefSoTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1de1960-3de0-4e73-b983-30408f4b45b8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final result of SVM on test dataset: 0.9586666666666667\n"
          ]
        }
      ]
    }
  ]
}