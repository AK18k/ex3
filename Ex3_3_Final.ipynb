{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AK18k/ex3/blob/main/Ex3_3_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AK18k/ex3\n"
      ],
      "metadata": {
        "id": "_gmAF3KS3EDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_PATH = '/content/drive/MyDrive/ex3/data'\n",
        "PATH = '/content/drive/MyDrive/ex3'\n"
      ],
      "metadata": {
        "id": "3qiSqOGVcGFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c3d0c4c-b700-4a7c-dc79-22f8e225ad8b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.modules.activation import Softplus\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# Define hyperparameters\n",
        "input_size = 28 * 28  # Size of the input images (28x28 pixels)\n",
        "latent_size = 50  # Length of the latent vector\n",
        "VAE_batch_size = 64\n",
        "SVM_batch_size = 64\n",
        "VAE_epochs = 3\n",
        "SVM_epochs = 10\n",
        "learning_rate = 1e-3\n",
        "num_hidden_units = 600\n",
        "num_of_labeled_samples = 100\n",
        "expansion_rate = 10\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "g3oK_4RagXtQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66866128-4d0d-4c2e-ef39-3e2b40898018"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6a2468edb0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load FashionMNIST dataset\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_dataset = datasets.FashionMNIST(root='data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.FashionMNIST(root=DATA_PATH, train=False, transform=transform, download=True)"
      ],
      "metadata": {
        "id": "w0CKdNquiISo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ada8e4c-1dc7-43ad-9c28-16373d9f7cc1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 8530789.55it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 138785.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 2647413.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 6422450.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /content/drive/MyDrive/ex3/data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 7958038.28it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/ex3/data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /content/drive/MyDrive/ex3/data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /content/drive/MyDrive/ex3/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 139427.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/ex3/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /content/drive/MyDrive/ex3/data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /content/drive/MyDrive/ex3/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 2592595.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/ex3/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /content/drive/MyDrive/ex3/data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /content/drive/MyDrive/ex3/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 7866038.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/ex3/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /content/drive/MyDrive/ex3/data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_to_labeled(original_dataset, num_of_labeled_samples):\n",
        "  class_count = {}\n",
        "  for _, label in original_dataset:\n",
        "      if label in class_count:\n",
        "          class_count[label] += 1\n",
        "      else:\n",
        "          class_count[label] = 1\n",
        "\n",
        "  # Calculate the desired number of samples for each class in the new dataset\n",
        "  samples_per_class = num_of_labeled_samples // len(class_count)\n",
        "\n",
        "  # Create a list to store the selected samples\n",
        "  selected_samples = []\n",
        "\n",
        "  # Iterate through the original dataset and select the desired number of samples from each class\n",
        "  selected_count = {label: 0 for label in class_count.keys()}\n",
        "  for data, label in original_dataset:\n",
        "      if selected_count[label] < samples_per_class:\n",
        "          selected_samples.append((data, label))\n",
        "          selected_count[label] += 1\n",
        "\n",
        "  class NewDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.data = [data for data, _ in samples]\n",
        "        self.labels = [label for _, label in samples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.labels[index]\n",
        "\n",
        "  # Create an instance of the new dataset using the selected samples\n",
        "  labled_dataset = NewDataset(selected_samples)\n",
        "\n",
        "  return labled_dataset\n",
        "\n",
        "def reparameterize(mu, logvar):\n",
        "    std = torch.exp(0.5 * logvar)\n",
        "    eps = torch.randn_like(std)\n",
        "    z = mu + eps * std\n",
        "    return z\n"
      ],
      "metadata": {
        "id": "FMxqzWxpVu-j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################################\n",
        "# Create and train the VAE model\n",
        "# Input:\n",
        "#   - data_loader - a dataloader with images and labels\n",
        "# Output:\n",
        "#   - the VAE model\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "# Define the VAE architecture\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_size, latent_size):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, num_hidden_units),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(num_hidden_units, latent_size * 2)  # Output mu and logvar for each latent dimension\n",
        "        )\n",
        "\n",
        "        # Decoder layers\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_size, num_hidden_units),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(num_hidden_units, input_size),\n",
        "            nn.Sigmoid()  # Output values between 0 and 1\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        return z\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        encoded = self.encoder(x)\n",
        "        mu, logvar = torch.chunk(encoded, 2, dim=1)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # Decoder\n",
        "        reconstructed = self.decoder(z)\n",
        "        return reconstructed, mu, logvar\n",
        "\n",
        "\n",
        "def train_VAE(data_loader):\n",
        "  # Create VAE model\n",
        "  VAE_model = VAE(input_size, latent_size).to(device)\n",
        "\n",
        "  # Define loss function\n",
        "  criterion = nn.BCELoss(reduction='sum')  # Binary cross-entropy loss\n",
        "\n",
        "  # Define optimizer\n",
        "  optimizer = optim.Adam(VAE_model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(VAE_epochs):\n",
        "      for i, (images, _) in enumerate(data_loader):\n",
        "          # Flatten input images\n",
        "          images = images.view(images.size(0), -1).to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          reconstructed, mu, logvar = VAE_model(images)\n",
        "\n",
        "          # Compute reconstruction loss and KL divergence\n",
        "          reconstruction_loss = criterion(reconstructed, images)\n",
        "          kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "          # Total loss\n",
        "          loss = reconstruction_loss + kl_divergence\n",
        "\n",
        "          # Backward and optimize\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 100 == 0:\n",
        "              print(f\"VAE train Epoch [{epoch+1}/{VAE_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "  torch.save(VAE_model.state_dict(), 'VAE_model.pth')\n",
        "  print('model saved')\n",
        "\n",
        "  return VAE_model\n",
        "\n"
      ],
      "metadata": {
        "id": "aokcicqztgDQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################\n",
        "# Passes a dataset of images through a pretrained VAE model\n",
        "# Inputs:\n",
        "#   - VAE_model\n",
        "#   - data - the dataset (not a dataloader)\n",
        "# Output:\n",
        "#   - output_vectors - latent vectors\n",
        "###############################################################\n",
        "\n",
        "def images_to_latent(VAE_model, data):\n",
        "  #data_loader = DataLoader(data, shuffle=False) # Ofer removed batch_size=VAE_batch_size because there is no training\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "  VAE_model.eval()\n",
        "\n",
        "  # Create an empty list to store the output vectors\n",
        "  z_output_vectors = []\n",
        "  mu_output_vectors = []\n",
        "  logvar_output_vectors = []\n",
        "  z_output_vectors.to(device)\n",
        "  mu_output_vectors.to(device)\n",
        "  logvar_output_vectors.to(device)\n",
        "  # Pass the dataset through the VAE model\n",
        "  with torch.no_grad():\n",
        "      for images, _ in data_loader:\n",
        "          images = images.to(device)\n",
        "          # Obtain the output vectors from the VAE model\n",
        "          z, mu, logvar = VAE_model(data_loader)\n",
        "          z_output_vectors.append(z)\n",
        "          mu_output_vectors.append(mu)\n",
        "          logvar_output_vectors.append(logvar)\n",
        "\n",
        "  # Concatenate the output vectors into a single tensor\n",
        "  output_vectors = torch.cat(output_vectors, dim=0)\n",
        "\n",
        "  return z_output_vectors, mu_output_vectors, logvar_output_vectors"
      ],
      "metadata": {
        "id": "tp_TWV9lrjQm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def SVM_model(kernel='poly', degree=3):\n",
        "  SVM_model = SVC(kernel=kernel, degree=degree)\n",
        "  return SVM_model\n",
        "\n",
        "def train_SVM(SVM_model, train_dataset):\n",
        "  data = train_dataset.data\n",
        "  data = data.cpu()\n",
        "  data_np = data.detach().numpy()\n",
        "\n",
        "  label = train_dataset.targets\n",
        "  label = label.cpu()\n",
        "  label_np = label.detach().numpy()\n",
        "  SVM_model.fit(data_np, label_np)\n",
        "\n",
        "  return SVM_model\n",
        "\n",
        "def test_SVM(SVM_model, test_dataset):\n",
        "  data = test_dataset.data\n",
        "  data = data.cpu()\n",
        "  data_np = data.detach().numpy()\n",
        "\n",
        "  label = test_dataset.targets\n",
        "  label = label.cpu()\n",
        "  label_np = label.detach().numpy()\n",
        "  accuracy = SVM_model.score(data_np, label_np)\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EMUm_73u7pRW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reparameterize(mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        return z"
      ],
      "metadata": {
        "id": "p1KQGy5orDrb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the VAE model with train dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=VAE_batch_size, shuffle=True)\n",
        "VAE_model = train_VAE(train_loader)\n",
        "\n",
        "# Split the train dataset\n",
        "print(f'Spliting the training dataset to {num_of_labeled_samples} labled samples')\n",
        "labled_dataset = split_to_labeled(train_dataset, num_of_labeled_samples)\n",
        "\n",
        "#create the expanded latent vector space\n",
        "print('Learning mu and logvar from labeled dataset')\n",
        "data_loader = DataLoader(labled_dataset, batch_size=VAE_batch_size, shuffle=False)\n",
        "for batch in data_loader:\n",
        "    images, _ = batch  # Assuming you don't need the labels\n",
        "    images = images.view(images.size(0), -1).to(device)\n",
        "    _, mu, logvar = VAE_model(images)\n",
        "\n",
        "z_data = torch.zeros([0,50]).to(device)\n",
        "z_label = torch.zeros([0]).to(device)\n",
        "\n",
        "print(f'Expanding the labeled dataset by {expansion_rate}')\n",
        "for i in range(0,len(mu)):\n",
        "  for e in range(0,1):\n",
        "    new_z = reparameterize(mu[i], logvar[i])\n",
        "    new_z = (new_z.unsqueeze(0))\n",
        "    new_label = torch.tensor(labled_dataset.labels[i]).to(device)\n",
        "    new_label = (new_label.unsqueeze(0))\n",
        "    z_data = torch.cat((z_data, new_z), 0)\n",
        "    z_label = torch.cat((z_label, new_label), 0).to(torch.int)\n",
        "\n",
        "print(f'Total entries to SVM train: {z_data.size()}')\n",
        "\n",
        "SVM = SVM_model(kernel='poly', degree=3)\n",
        "\n",
        "print('Training the SVM with expanded latent dataset')\n",
        "latent_dataset = Dataset()\n",
        "latent_dataset.data = z_data\n",
        "latent_dataset.targets = z_label\n",
        "\n",
        "SVM = train_SVM(SVM, latent_dataset)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cfQvO0n6wsSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Learning mu and logvar from labeled dataset')\n",
        "data_loader = DataLoader(test_dataset, batch_size=VAE_batch_size, shuffle=False)\n",
        "for batch in data_loader:\n",
        "    images, _ = batch  # Assuming you don't need the labels\n",
        "    images = images.view(images.size(0), -1).to(device)\n",
        "    _, mu, logvar = VAE_model(images)\n",
        "\n",
        "z_data = torch.zeros([0,50]).to(device)\n",
        "z_label = torch.zeros([0]).to(device)\n",
        "\n",
        "for i in range(len(test_dataset)):\n",
        "  new_z = reparameterize(mu[i], logvar[i])\n",
        "  new_z = (new_z.unsqueeze(0))\n",
        "  new_label = torch.tensor(labled_dataset.labels[i]).to(device)\n",
        "  new_label = (new_label.unsqueeze(0))\n",
        "  z_data = torch.cat((z_data, new_z), 0)\n",
        "  z_label = torch.cat((z_label, new_label), 0).to(torch.int)\n",
        "\n",
        "latent_dataset = Dataset()\n",
        "latent_dataset.data = z_data\n",
        "latent_dataset.targets = z_label\n",
        "\n",
        "\n",
        "result = test_SVM(SVM, latent_dataset)\n",
        "\n",
        "print(f'Final result of SVM on test dataset: {result}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "e2CgMKkftZ2D",
        "outputId": "9be31fb0-5740-4f30-dbc1-0ffa0acce844"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning mu and logvar from labeled dataset\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-23a795cddd25>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mnew_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mnew_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mnew_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabled_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 16 is out of bounds for dimension 0 with size 16"
          ]
        }
      ]
    }
  ]
}