{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AK18k/ex3/blob/main/Ex3_3_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AK18k/ex3\n"
      ],
      "metadata": {
        "id": "_gmAF3KS3EDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "407c47aa-4559-42f5-ec29-e21d1528ae32"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ex3'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 40 (delta 20), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (40/40), 29.46 MiB | 1.38 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "DATA_PATH = '/content/drive/ex3/data'\n",
        "PATH = '/content/drive/ex3'\n"
      ],
      "metadata": {
        "id": "3qiSqOGVcGFy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.modules.activation import Softplus\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# Define hyperparameters\n",
        "input_size = 28 * 28  # Size of the input images (28x28 pixels)\n",
        "latent_size = 50  # Length of the latent vector\n",
        "VAE_batch_size = 64\n",
        "SVM_batch_size = 64\n",
        "VAE_epochs = 50\n",
        "SVM_epochs = 10\n",
        "learning_rate = 1e-3\n",
        "num_hidden_units = 600\n",
        "num_of_labeled_samples = 100\n",
        "expansion_rate = 10\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "g3oK_4RagXtQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a827c9e7-5f11-447b-8a90-9b32b8ad8b80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fdf7f6e6c30>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load FashionMNIST dataset\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_dataset = datasets.FashionMNIST(root='data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.FashionMNIST(root=DATA_PATH, train=False, transform=transform, download=True)"
      ],
      "metadata": {
        "id": "w0CKdNquiISo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "997d41d8-ac97-4a04-ca01-937f5eabda4d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 8263288.29it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 138108.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 2638053.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 5661320.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /content/drive/ex3/data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 8277128.46it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/ex3/data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /content/drive/ex3/data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /content/drive/ex3/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 139815.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/ex3/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /content/drive/ex3/data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /content/drive/ex3/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 2648180.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/ex3/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /content/drive/ex3/data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /content/drive/ex3/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 8424610.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/ex3/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /content/drive/ex3/data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_to_labeled(original_dataset, num_of_labeled_samples):\n",
        "  class_count = {}\n",
        "  for _, label in original_dataset:\n",
        "      if label in class_count:\n",
        "          class_count[label] += 1\n",
        "      else:\n",
        "          class_count[label] = 1\n",
        "\n",
        "  # Calculate the desired number of samples for each class in the new dataset\n",
        "  samples_per_class = num_of_labeled_samples // len(class_count)\n",
        "\n",
        "  # Create a list to store the selected samples\n",
        "  selected_samples = []\n",
        "\n",
        "  # Iterate through the original dataset and select the desired number of samples from each class\n",
        "  selected_count = {label: 0 for label in class_count.keys()}\n",
        "  for data, label in original_dataset:\n",
        "      if selected_count[label] < samples_per_class:\n",
        "          selected_samples.append((data, label))\n",
        "          selected_count[label] += 1\n",
        "\n",
        "  class NewDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.data = [data for data, _ in samples]\n",
        "        self.labels = [label for _, label in samples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.labels[index]\n",
        "\n",
        "  # Create an instance of the new dataset using the selected samples\n",
        "  labled_dataset = NewDataset(selected_samples)\n",
        "\n",
        "  return labled_dataset\n",
        "\n",
        "def reparameterize(mu, logvar):\n",
        "    std = torch.exp(0.5 * logvar)\n",
        "    eps = torch.randn_like(std)\n",
        "    z = mu + eps * std\n",
        "    return z\n"
      ],
      "metadata": {
        "id": "FMxqzWxpVu-j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################################\n",
        "# Create and train the VAE model\n",
        "# Input:\n",
        "#   - data_loader - a dataloader with images and labels\n",
        "# Output:\n",
        "#   - the VAE model\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "# Define the VAE architecture\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_size, latent_size):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, num_hidden_units),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(num_hidden_units, latent_size * 2)  # Output mu and logvar for each latent dimension\n",
        "        )\n",
        "\n",
        "        # Decoder layers\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_size, num_hidden_units),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(num_hidden_units, input_size),\n",
        "            nn.Sigmoid()  # Output values between 0 and 1\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        return z\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        encoded = self.encoder(x)\n",
        "        mu, logvar = torch.chunk(encoded, 2, dim=1)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # Decoder\n",
        "        reconstructed = self.decoder(z)\n",
        "        return reconstructed, mu, logvar\n",
        "\n",
        "\n",
        "def train_VAE(data_loader):\n",
        "  # Create VAE model\n",
        "  VAE_model = VAE(input_size, latent_size).to(device)\n",
        "\n",
        "  # Define loss function\n",
        "  criterion = nn.BCELoss(reduction='sum')  # Binary cross-entropy loss\n",
        "\n",
        "  # Define optimizer\n",
        "  optimizer = optim.Adam(VAE_model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(VAE_epochs):\n",
        "      for i, (images, _) in enumerate(data_loader):\n",
        "          # Flatten input images\n",
        "          images = images.view(images.size(0), -1).to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          reconstructed, mu, logvar = VAE_model(images)\n",
        "\n",
        "          # Compute reconstruction loss and KL divergence\n",
        "          reconstruction_loss = criterion(reconstructed, images)\n",
        "          kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "          # Total loss\n",
        "          loss = reconstruction_loss + kl_divergence\n",
        "\n",
        "          # Backward and optimize\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 100 == 0:\n",
        "              print(f\"VAE train Epoch [{epoch+1}/{VAE_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "  torch.save(VAE_model.state_dict(), 'VAE_model.pth')\n",
        "  print('model saved')\n",
        "\n",
        "  return VAE_model\n",
        "\n"
      ],
      "metadata": {
        "id": "aokcicqztgDQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################\n",
        "# Passes a dataset of images through a pretrained VAE model\n",
        "# Inputs:\n",
        "#   - VAE_model\n",
        "#   - data - the dataset (not a dataloader)\n",
        "# Output:\n",
        "#   - output_vectors - latent vectors\n",
        "###############################################################\n",
        "\n",
        "def images_to_latent(VAE_model, data):\n",
        "  #data_loader = DataLoader(data, shuffle=False) # Ofer removed batch_size=VAE_batch_size because there is no training\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "  VAE_model.eval()\n",
        "\n",
        "  # Create an empty list to store the output vectors\n",
        "  z_output_vectors = []\n",
        "  mu_output_vectors = []\n",
        "  logvar_output_vectors = []\n",
        "  z_output_vectors.to(device)\n",
        "  mu_output_vectors.to(device)\n",
        "  logvar_output_vectors.to(device)\n",
        "  # Pass the dataset through the VAE model\n",
        "  with torch.no_grad():\n",
        "      for images, _ in data_loader:\n",
        "          images = images.to(device)\n",
        "          # Obtain the output vectors from the VAE model\n",
        "          z, mu, logvar = VAE_model(data_loader)\n",
        "          z_output_vectors.append(z)\n",
        "          mu_output_vectors.append(mu)\n",
        "          logvar_output_vectors.append(logvar)\n",
        "\n",
        "  # Concatenate the output vectors into a single tensor\n",
        "  output_vectors = torch.cat(output_vectors, dim=0)\n",
        "\n",
        "  return z_output_vectors, mu_output_vectors, logvar_output_vectors"
      ],
      "metadata": {
        "id": "tp_TWV9lrjQm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def SVM_model(kernel='poly', degree=3):\n",
        "  SVM_model = SVC(kernel=kernel, degree=degree)\n",
        "  return SVM_model\n",
        "\n",
        "def train_SVM(SVM_model, train_dataset):\n",
        "  data = train_dataset.data\n",
        "  data = data.cpu()\n",
        "  data_np = data.detach().numpy()\n",
        "\n",
        "  label = train_dataset.targets\n",
        "  label = label.cpu()\n",
        "  label_np = label.detach().numpy()\n",
        "  SVM_model.fit(data_np, label_np)\n",
        "\n",
        "  return SVM_model\n",
        "\n",
        "def test_SVM(SVM_model, test_dataset):\n",
        "  data = test_dataset.data\n",
        "  data = data.cpu()\n",
        "  data_np = data.detach().numpy()\n",
        "\n",
        "  label = test_dataset.targets\n",
        "  label = label.cpu()\n",
        "  label_np = label.detach().numpy()\n",
        "  accuracy = SVM_model.score(data_np, label_np)\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EMUm_73u7pRW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reparameterize(mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        return z"
      ],
      "metadata": {
        "id": "p1KQGy5orDrb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=VAE_batch_size, shuffle=True)\n",
        "VAE_model = train_VAE(train_loader)\n"
      ],
      "metadata": {
        "id": "cfQvO0n6wsSH",
        "outputId": "8c2a8f01-21eb-4910-cbe1-2de5f18555b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAE train Epoch [1/50], Step [100/938], Loss: 19497.8008\n",
            "VAE train Epoch [1/50], Step [200/938], Loss: 17816.2012\n",
            "VAE train Epoch [1/50], Step [300/938], Loss: 18472.3418\n",
            "VAE train Epoch [1/50], Step [400/938], Loss: 17867.4375\n",
            "VAE train Epoch [1/50], Step [500/938], Loss: 17290.5273\n",
            "VAE train Epoch [1/50], Step [600/938], Loss: 17072.5996\n",
            "VAE train Epoch [1/50], Step [700/938], Loss: 16384.9180\n",
            "VAE train Epoch [1/50], Step [800/938], Loss: 17532.1953\n",
            "VAE train Epoch [1/50], Step [900/938], Loss: 16192.9053\n",
            "VAE train Epoch [2/50], Step [100/938], Loss: 16466.1895\n",
            "VAE train Epoch [2/50], Step [200/938], Loss: 16445.3066\n",
            "VAE train Epoch [2/50], Step [300/938], Loss: 15480.3789\n",
            "VAE train Epoch [2/50], Step [400/938], Loss: 16704.9062\n",
            "VAE train Epoch [2/50], Step [500/938], Loss: 16403.5645\n",
            "VAE train Epoch [2/50], Step [600/938], Loss: 15405.5322\n",
            "VAE train Epoch [2/50], Step [700/938], Loss: 16952.8340\n",
            "VAE train Epoch [2/50], Step [800/938], Loss: 16279.2070\n",
            "VAE train Epoch [2/50], Step [900/938], Loss: 16308.4199\n",
            "VAE train Epoch [3/50], Step [100/938], Loss: 16640.6992\n",
            "VAE train Epoch [3/50], Step [200/938], Loss: 16078.4805\n",
            "VAE train Epoch [3/50], Step [300/938], Loss: 16899.1055\n",
            "VAE train Epoch [3/50], Step [400/938], Loss: 16393.4238\n",
            "VAE train Epoch [3/50], Step [500/938], Loss: 14730.1416\n",
            "VAE train Epoch [3/50], Step [600/938], Loss: 16773.0605\n",
            "VAE train Epoch [3/50], Step [700/938], Loss: 15974.3281\n",
            "VAE train Epoch [3/50], Step [800/938], Loss: 15747.2852\n",
            "VAE train Epoch [3/50], Step [900/938], Loss: 16063.8662\n",
            "VAE train Epoch [4/50], Step [100/938], Loss: 15799.5371\n",
            "VAE train Epoch [4/50], Step [200/938], Loss: 15398.2197\n",
            "VAE train Epoch [4/50], Step [300/938], Loss: 15559.6260\n",
            "VAE train Epoch [4/50], Step [400/938], Loss: 16309.3496\n",
            "VAE train Epoch [4/50], Step [500/938], Loss: 15069.3408\n",
            "VAE train Epoch [4/50], Step [600/938], Loss: 15859.7490\n",
            "VAE train Epoch [4/50], Step [700/938], Loss: 15155.2139\n",
            "VAE train Epoch [4/50], Step [800/938], Loss: 15629.1572\n",
            "VAE train Epoch [4/50], Step [900/938], Loss: 15713.7119\n",
            "VAE train Epoch [5/50], Step [100/938], Loss: 16109.4814\n",
            "VAE train Epoch [5/50], Step [200/938], Loss: 16109.3389\n",
            "VAE train Epoch [5/50], Step [300/938], Loss: 14551.5078\n",
            "VAE train Epoch [5/50], Step [400/938], Loss: 15284.0625\n",
            "VAE train Epoch [5/50], Step [500/938], Loss: 16005.5332\n",
            "VAE train Epoch [5/50], Step [600/938], Loss: 16149.2051\n",
            "VAE train Epoch [5/50], Step [700/938], Loss: 15662.4277\n",
            "VAE train Epoch [5/50], Step [800/938], Loss: 16818.4434\n",
            "VAE train Epoch [5/50], Step [900/938], Loss: 14392.2969\n",
            "VAE train Epoch [6/50], Step [100/938], Loss: 16421.7090\n",
            "VAE train Epoch [6/50], Step [200/938], Loss: 15638.6689\n",
            "VAE train Epoch [6/50], Step [300/938], Loss: 15370.4678\n",
            "VAE train Epoch [6/50], Step [400/938], Loss: 15367.9766\n",
            "VAE train Epoch [6/50], Step [500/938], Loss: 15393.4980\n",
            "VAE train Epoch [6/50], Step [600/938], Loss: 16636.9297\n",
            "VAE train Epoch [6/50], Step [700/938], Loss: 15351.4775\n",
            "VAE train Epoch [6/50], Step [800/938], Loss: 15301.0293\n",
            "VAE train Epoch [6/50], Step [900/938], Loss: 15423.0127\n",
            "VAE train Epoch [7/50], Step [100/938], Loss: 15948.0420\n",
            "VAE train Epoch [7/50], Step [200/938], Loss: 15005.7676\n",
            "VAE train Epoch [7/50], Step [300/938], Loss: 15940.3730\n",
            "VAE train Epoch [7/50], Step [400/938], Loss: 16857.3672\n",
            "VAE train Epoch [7/50], Step [500/938], Loss: 15521.5010\n",
            "VAE train Epoch [7/50], Step [600/938], Loss: 15717.9795\n",
            "VAE train Epoch [7/50], Step [700/938], Loss: 15471.7236\n",
            "VAE train Epoch [7/50], Step [800/938], Loss: 15238.0098\n",
            "VAE train Epoch [7/50], Step [900/938], Loss: 16429.4941\n",
            "VAE train Epoch [8/50], Step [100/938], Loss: 15791.7129\n",
            "VAE train Epoch [8/50], Step [200/938], Loss: 15659.9482\n",
            "VAE train Epoch [8/50], Step [300/938], Loss: 15971.3633\n",
            "VAE train Epoch [8/50], Step [400/938], Loss: 16168.2422\n",
            "VAE train Epoch [8/50], Step [500/938], Loss: 15661.8096\n",
            "VAE train Epoch [8/50], Step [600/938], Loss: 16074.6797\n",
            "VAE train Epoch [8/50], Step [700/938], Loss: 15533.3408\n",
            "VAE train Epoch [8/50], Step [800/938], Loss: 15521.4141\n",
            "VAE train Epoch [8/50], Step [900/938], Loss: 16197.6240\n",
            "VAE train Epoch [9/50], Step [100/938], Loss: 15296.8887\n",
            "VAE train Epoch [9/50], Step [200/938], Loss: 15526.9531\n",
            "VAE train Epoch [9/50], Step [300/938], Loss: 14653.6475\n",
            "VAE train Epoch [9/50], Step [400/938], Loss: 16142.6113\n",
            "VAE train Epoch [9/50], Step [500/938], Loss: 16286.0098\n",
            "VAE train Epoch [9/50], Step [600/938], Loss: 16885.6426\n",
            "VAE train Epoch [9/50], Step [700/938], Loss: 15106.7871\n",
            "VAE train Epoch [9/50], Step [800/938], Loss: 16044.7676\n",
            "VAE train Epoch [9/50], Step [900/938], Loss: 14402.0410\n",
            "VAE train Epoch [10/50], Step [100/938], Loss: 15146.9453\n",
            "VAE train Epoch [10/50], Step [200/938], Loss: 15769.0039\n",
            "VAE train Epoch [10/50], Step [300/938], Loss: 16759.4219\n",
            "VAE train Epoch [10/50], Step [400/938], Loss: 16243.6523\n",
            "VAE train Epoch [10/50], Step [500/938], Loss: 16458.4668\n",
            "VAE train Epoch [10/50], Step [600/938], Loss: 16360.5508\n",
            "VAE train Epoch [10/50], Step [700/938], Loss: 15247.6367\n",
            "VAE train Epoch [10/50], Step [800/938], Loss: 16714.1875\n",
            "VAE train Epoch [10/50], Step [900/938], Loss: 16221.2324\n",
            "VAE train Epoch [11/50], Step [100/938], Loss: 14410.1895\n",
            "VAE train Epoch [11/50], Step [200/938], Loss: 14391.5537\n",
            "VAE train Epoch [11/50], Step [300/938], Loss: 15771.6455\n",
            "VAE train Epoch [11/50], Step [400/938], Loss: 14491.9600\n",
            "VAE train Epoch [11/50], Step [500/938], Loss: 16461.1504\n",
            "VAE train Epoch [11/50], Step [600/938], Loss: 14789.0371\n",
            "VAE train Epoch [11/50], Step [700/938], Loss: 16311.9600\n",
            "VAE train Epoch [11/50], Step [800/938], Loss: 16001.2725\n",
            "VAE train Epoch [11/50], Step [900/938], Loss: 14774.2207\n",
            "VAE train Epoch [12/50], Step [100/938], Loss: 16086.6436\n",
            "VAE train Epoch [12/50], Step [200/938], Loss: 15849.5166\n",
            "VAE train Epoch [12/50], Step [300/938], Loss: 14202.6816\n",
            "VAE train Epoch [12/50], Step [400/938], Loss: 16091.5771\n",
            "VAE train Epoch [12/50], Step [500/938], Loss: 15441.9180\n",
            "VAE train Epoch [12/50], Step [600/938], Loss: 16010.1729\n",
            "VAE train Epoch [12/50], Step [700/938], Loss: 16652.7715\n",
            "VAE train Epoch [12/50], Step [800/938], Loss: 16468.0430\n",
            "VAE train Epoch [12/50], Step [900/938], Loss: 15776.5430\n",
            "VAE train Epoch [13/50], Step [100/938], Loss: 15273.9160\n",
            "VAE train Epoch [13/50], Step [200/938], Loss: 15377.8047\n",
            "VAE train Epoch [13/50], Step [300/938], Loss: 15418.4873\n",
            "VAE train Epoch [13/50], Step [400/938], Loss: 14086.3154\n",
            "VAE train Epoch [13/50], Step [500/938], Loss: 15470.6309\n",
            "VAE train Epoch [13/50], Step [600/938], Loss: 15596.9551\n",
            "VAE train Epoch [13/50], Step [700/938], Loss: 15711.8916\n",
            "VAE train Epoch [13/50], Step [800/938], Loss: 15261.9805\n",
            "VAE train Epoch [13/50], Step [900/938], Loss: 15415.7998\n",
            "VAE train Epoch [14/50], Step [100/938], Loss: 15101.7646\n",
            "VAE train Epoch [14/50], Step [200/938], Loss: 15629.8945\n",
            "VAE train Epoch [14/50], Step [300/938], Loss: 14967.9316\n",
            "VAE train Epoch [14/50], Step [400/938], Loss: 15655.1396\n",
            "VAE train Epoch [14/50], Step [500/938], Loss: 16312.7988\n",
            "VAE train Epoch [14/50], Step [600/938], Loss: 15126.1367\n",
            "VAE train Epoch [14/50], Step [700/938], Loss: 15690.7080\n",
            "VAE train Epoch [14/50], Step [800/938], Loss: 14825.6621\n",
            "VAE train Epoch [14/50], Step [900/938], Loss: 15422.8340\n",
            "VAE train Epoch [15/50], Step [100/938], Loss: 15225.9922\n",
            "VAE train Epoch [15/50], Step [200/938], Loss: 16118.3271\n",
            "VAE train Epoch [15/50], Step [300/938], Loss: 16565.3906\n",
            "VAE train Epoch [15/50], Step [400/938], Loss: 14611.5479\n",
            "VAE train Epoch [15/50], Step [500/938], Loss: 14695.8574\n",
            "VAE train Epoch [15/50], Step [600/938], Loss: 16421.9043\n",
            "VAE train Epoch [15/50], Step [700/938], Loss: 15905.3672\n",
            "VAE train Epoch [15/50], Step [800/938], Loss: 14554.1250\n",
            "VAE train Epoch [15/50], Step [900/938], Loss: 15870.2734\n",
            "VAE train Epoch [16/50], Step [100/938], Loss: 15679.1533\n",
            "VAE train Epoch [16/50], Step [200/938], Loss: 15975.1621\n",
            "VAE train Epoch [16/50], Step [300/938], Loss: 14822.8379\n",
            "VAE train Epoch [16/50], Step [400/938], Loss: 15413.5449\n",
            "VAE train Epoch [16/50], Step [500/938], Loss: 16443.5039\n",
            "VAE train Epoch [16/50], Step [600/938], Loss: 14974.4678\n",
            "VAE train Epoch [16/50], Step [700/938], Loss: 15818.0264\n",
            "VAE train Epoch [16/50], Step [800/938], Loss: 15089.3818\n",
            "VAE train Epoch [16/50], Step [900/938], Loss: 15941.2031\n",
            "VAE train Epoch [17/50], Step [100/938], Loss: 15680.0518\n",
            "VAE train Epoch [17/50], Step [200/938], Loss: 14467.6924\n",
            "VAE train Epoch [17/50], Step [300/938], Loss: 14573.8066\n",
            "VAE train Epoch [17/50], Step [400/938], Loss: 15223.9473\n",
            "VAE train Epoch [17/50], Step [500/938], Loss: 15064.3203\n",
            "VAE train Epoch [17/50], Step [600/938], Loss: 15439.1572\n",
            "VAE train Epoch [17/50], Step [700/938], Loss: 15195.4404\n",
            "VAE train Epoch [17/50], Step [800/938], Loss: 14799.4160\n",
            "VAE train Epoch [17/50], Step [900/938], Loss: 15927.6602\n",
            "VAE train Epoch [18/50], Step [100/938], Loss: 14939.3877\n",
            "VAE train Epoch [18/50], Step [200/938], Loss: 15974.4600\n",
            "VAE train Epoch [18/50], Step [300/938], Loss: 15619.4014\n",
            "VAE train Epoch [18/50], Step [400/938], Loss: 14959.6113\n",
            "VAE train Epoch [18/50], Step [500/938], Loss: 14915.4453\n",
            "VAE train Epoch [18/50], Step [600/938], Loss: 15542.0986\n",
            "VAE train Epoch [18/50], Step [700/938], Loss: 14801.6191\n",
            "VAE train Epoch [18/50], Step [800/938], Loss: 14485.5137\n",
            "VAE train Epoch [18/50], Step [900/938], Loss: 15341.3037\n",
            "VAE train Epoch [19/50], Step [100/938], Loss: 16556.0410\n",
            "VAE train Epoch [19/50], Step [200/938], Loss: 14836.0557\n",
            "VAE train Epoch [19/50], Step [300/938], Loss: 14400.8389\n",
            "VAE train Epoch [19/50], Step [400/938], Loss: 15708.6318\n",
            "VAE train Epoch [19/50], Step [500/938], Loss: 16612.3730\n",
            "VAE train Epoch [19/50], Step [600/938], Loss: 15637.7646\n",
            "VAE train Epoch [19/50], Step [700/938], Loss: 15092.5020\n",
            "VAE train Epoch [19/50], Step [800/938], Loss: 15367.4609\n",
            "VAE train Epoch [19/50], Step [900/938], Loss: 15938.1357\n",
            "VAE train Epoch [20/50], Step [100/938], Loss: 15974.9111\n",
            "VAE train Epoch [20/50], Step [200/938], Loss: 15309.4268\n",
            "VAE train Epoch [20/50], Step [300/938], Loss: 15962.9023\n",
            "VAE train Epoch [20/50], Step [400/938], Loss: 15420.8604\n",
            "VAE train Epoch [20/50], Step [500/938], Loss: 14686.9355\n",
            "VAE train Epoch [20/50], Step [600/938], Loss: 16285.7539\n",
            "VAE train Epoch [20/50], Step [700/938], Loss: 16285.4824\n",
            "VAE train Epoch [20/50], Step [800/938], Loss: 15003.9756\n",
            "VAE train Epoch [20/50], Step [900/938], Loss: 14719.4980\n",
            "VAE train Epoch [21/50], Step [100/938], Loss: 14784.7783\n",
            "VAE train Epoch [21/50], Step [200/938], Loss: 15702.5645\n",
            "VAE train Epoch [21/50], Step [300/938], Loss: 15748.6152\n",
            "VAE train Epoch [21/50], Step [400/938], Loss: 14894.9229\n",
            "VAE train Epoch [21/50], Step [500/938], Loss: 15232.9414\n",
            "VAE train Epoch [21/50], Step [600/938], Loss: 15758.0410\n",
            "VAE train Epoch [21/50], Step [700/938], Loss: 14338.5400\n",
            "VAE train Epoch [21/50], Step [800/938], Loss: 15794.0117\n",
            "VAE train Epoch [21/50], Step [900/938], Loss: 15649.5781\n",
            "VAE train Epoch [22/50], Step [100/938], Loss: 14370.8271\n",
            "VAE train Epoch [22/50], Step [200/938], Loss: 16109.0117\n",
            "VAE train Epoch [22/50], Step [300/938], Loss: 15801.0059\n",
            "VAE train Epoch [22/50], Step [400/938], Loss: 14696.4424\n",
            "VAE train Epoch [22/50], Step [500/938], Loss: 16107.2070\n",
            "VAE train Epoch [22/50], Step [600/938], Loss: 15504.5049\n",
            "VAE train Epoch [22/50], Step [700/938], Loss: 16074.8291\n",
            "VAE train Epoch [22/50], Step [800/938], Loss: 14869.9521\n",
            "VAE train Epoch [22/50], Step [900/938], Loss: 15440.5781\n",
            "VAE train Epoch [23/50], Step [100/938], Loss: 14805.9287\n",
            "VAE train Epoch [23/50], Step [200/938], Loss: 15971.4053\n",
            "VAE train Epoch [23/50], Step [300/938], Loss: 14418.2988\n",
            "VAE train Epoch [23/50], Step [400/938], Loss: 14124.2549\n",
            "VAE train Epoch [23/50], Step [500/938], Loss: 15364.8447\n",
            "VAE train Epoch [23/50], Step [600/938], Loss: 16350.1357\n",
            "VAE train Epoch [23/50], Step [700/938], Loss: 15406.2344\n",
            "VAE train Epoch [23/50], Step [800/938], Loss: 14855.0420\n",
            "VAE train Epoch [23/50], Step [900/938], Loss: 15726.7539\n",
            "VAE train Epoch [24/50], Step [100/938], Loss: 16110.6113\n",
            "VAE train Epoch [24/50], Step [200/938], Loss: 15664.5049\n",
            "VAE train Epoch [24/50], Step [300/938], Loss: 14663.4033\n",
            "VAE train Epoch [24/50], Step [400/938], Loss: 15857.8369\n",
            "VAE train Epoch [24/50], Step [500/938], Loss: 14214.7324\n",
            "VAE train Epoch [24/50], Step [600/938], Loss: 15127.7051\n",
            "VAE train Epoch [24/50], Step [700/938], Loss: 14452.0635\n",
            "VAE train Epoch [24/50], Step [800/938], Loss: 14228.3340\n",
            "VAE train Epoch [24/50], Step [900/938], Loss: 15173.5479\n",
            "VAE train Epoch [25/50], Step [100/938], Loss: 15583.8516\n",
            "VAE train Epoch [25/50], Step [200/938], Loss: 14400.6279\n",
            "VAE train Epoch [25/50], Step [300/938], Loss: 15241.2002\n",
            "VAE train Epoch [25/50], Step [400/938], Loss: 14524.3027\n",
            "VAE train Epoch [25/50], Step [500/938], Loss: 15085.5615\n",
            "VAE train Epoch [25/50], Step [600/938], Loss: 16896.1641\n",
            "VAE train Epoch [25/50], Step [700/938], Loss: 14677.8389\n",
            "VAE train Epoch [25/50], Step [800/938], Loss: 15880.6416\n",
            "VAE train Epoch [25/50], Step [900/938], Loss: 16348.9668\n",
            "VAE train Epoch [26/50], Step [100/938], Loss: 15947.7715\n",
            "VAE train Epoch [26/50], Step [200/938], Loss: 15377.3984\n",
            "VAE train Epoch [26/50], Step [300/938], Loss: 16844.6934\n",
            "VAE train Epoch [26/50], Step [400/938], Loss: 15136.0459\n",
            "VAE train Epoch [26/50], Step [500/938], Loss: 16379.7344\n",
            "VAE train Epoch [26/50], Step [600/938], Loss: 16069.2529\n",
            "VAE train Epoch [26/50], Step [700/938], Loss: 16502.8789\n",
            "VAE train Epoch [26/50], Step [800/938], Loss: 14858.1494\n",
            "VAE train Epoch [26/50], Step [900/938], Loss: 15186.6758\n",
            "VAE train Epoch [27/50], Step [100/938], Loss: 15218.4531\n",
            "VAE train Epoch [27/50], Step [200/938], Loss: 15418.1221\n",
            "VAE train Epoch [27/50], Step [300/938], Loss: 16093.3389\n",
            "VAE train Epoch [27/50], Step [400/938], Loss: 14062.7939\n",
            "VAE train Epoch [27/50], Step [500/938], Loss: 14967.7852\n",
            "VAE train Epoch [27/50], Step [600/938], Loss: 16363.0811\n",
            "VAE train Epoch [27/50], Step [700/938], Loss: 16122.7627\n",
            "VAE train Epoch [27/50], Step [800/938], Loss: 16635.4805\n",
            "VAE train Epoch [27/50], Step [900/938], Loss: 15285.9287\n",
            "VAE train Epoch [28/50], Step [100/938], Loss: 15331.3584\n",
            "VAE train Epoch [28/50], Step [200/938], Loss: 15192.0889\n",
            "VAE train Epoch [28/50], Step [300/938], Loss: 15100.5078\n",
            "VAE train Epoch [28/50], Step [400/938], Loss: 16395.1270\n",
            "VAE train Epoch [28/50], Step [500/938], Loss: 15089.4795\n",
            "VAE train Epoch [28/50], Step [600/938], Loss: 15521.8623\n",
            "VAE train Epoch [28/50], Step [700/938], Loss: 15516.9785\n",
            "VAE train Epoch [28/50], Step [800/938], Loss: 14965.4463\n",
            "VAE train Epoch [28/50], Step [900/938], Loss: 16071.0127\n",
            "VAE train Epoch [29/50], Step [100/938], Loss: 15018.2129\n",
            "VAE train Epoch [29/50], Step [200/938], Loss: 15239.2324\n",
            "VAE train Epoch [29/50], Step [300/938], Loss: 15252.7441\n",
            "VAE train Epoch [29/50], Step [400/938], Loss: 15338.6641\n",
            "VAE train Epoch [29/50], Step [500/938], Loss: 14767.6621\n",
            "VAE train Epoch [29/50], Step [600/938], Loss: 15490.0879\n",
            "VAE train Epoch [29/50], Step [700/938], Loss: 15147.5068\n",
            "VAE train Epoch [29/50], Step [800/938], Loss: 15087.1934\n",
            "VAE train Epoch [29/50], Step [900/938], Loss: 14857.9609\n",
            "VAE train Epoch [30/50], Step [100/938], Loss: 15789.9922\n",
            "VAE train Epoch [30/50], Step [200/938], Loss: 14789.1738\n",
            "VAE train Epoch [30/50], Step [300/938], Loss: 14686.0645\n",
            "VAE train Epoch [30/50], Step [400/938], Loss: 16092.3682\n",
            "VAE train Epoch [30/50], Step [500/938], Loss: 15146.6064\n",
            "VAE train Epoch [30/50], Step [600/938], Loss: 16638.5762\n",
            "VAE train Epoch [30/50], Step [700/938], Loss: 15378.6475\n",
            "VAE train Epoch [30/50], Step [800/938], Loss: 15535.5654\n",
            "VAE train Epoch [30/50], Step [900/938], Loss: 14420.4980\n",
            "VAE train Epoch [31/50], Step [100/938], Loss: 15242.1699\n",
            "VAE train Epoch [31/50], Step [200/938], Loss: 15489.7422\n",
            "VAE train Epoch [31/50], Step [300/938], Loss: 14755.8008\n",
            "VAE train Epoch [31/50], Step [400/938], Loss: 15499.9258\n",
            "VAE train Epoch [31/50], Step [500/938], Loss: 15096.2207\n",
            "VAE train Epoch [31/50], Step [600/938], Loss: 14837.3701\n",
            "VAE train Epoch [31/50], Step [700/938], Loss: 14128.7080\n",
            "VAE train Epoch [31/50], Step [800/938], Loss: 15434.5723\n",
            "VAE train Epoch [31/50], Step [900/938], Loss: 15701.0400\n",
            "VAE train Epoch [32/50], Step [100/938], Loss: 15562.9502\n",
            "VAE train Epoch [32/50], Step [200/938], Loss: 14754.3076\n",
            "VAE train Epoch [32/50], Step [300/938], Loss: 14819.9873\n",
            "VAE train Epoch [32/50], Step [400/938], Loss: 16055.8857\n",
            "VAE train Epoch [32/50], Step [500/938], Loss: 16033.3613\n",
            "VAE train Epoch [32/50], Step [600/938], Loss: 16213.8438\n",
            "VAE train Epoch [32/50], Step [700/938], Loss: 15770.9707\n",
            "VAE train Epoch [32/50], Step [800/938], Loss: 15716.0742\n",
            "VAE train Epoch [32/50], Step [900/938], Loss: 15028.1387\n",
            "VAE train Epoch [33/50], Step [100/938], Loss: 15479.5459\n",
            "VAE train Epoch [33/50], Step [200/938], Loss: 15377.2510\n",
            "VAE train Epoch [33/50], Step [300/938], Loss: 15232.4951\n",
            "VAE train Epoch [33/50], Step [400/938], Loss: 15415.4238\n",
            "VAE train Epoch [33/50], Step [500/938], Loss: 15246.8027\n",
            "VAE train Epoch [33/50], Step [600/938], Loss: 15827.4414\n",
            "VAE train Epoch [33/50], Step [700/938], Loss: 15903.9688\n",
            "VAE train Epoch [33/50], Step [800/938], Loss: 15129.0371\n",
            "VAE train Epoch [33/50], Step [900/938], Loss: 14639.4678\n",
            "VAE train Epoch [34/50], Step [100/938], Loss: 15751.4961\n",
            "VAE train Epoch [34/50], Step [200/938], Loss: 14797.5781\n",
            "VAE train Epoch [34/50], Step [300/938], Loss: 14965.9082\n",
            "VAE train Epoch [34/50], Step [400/938], Loss: 17039.4453\n",
            "VAE train Epoch [34/50], Step [500/938], Loss: 14265.8213\n",
            "VAE train Epoch [34/50], Step [600/938], Loss: 15774.5166\n",
            "VAE train Epoch [34/50], Step [700/938], Loss: 14908.2656\n",
            "VAE train Epoch [34/50], Step [800/938], Loss: 15175.1992\n",
            "VAE train Epoch [34/50], Step [900/938], Loss: 14924.2168\n",
            "VAE train Epoch [35/50], Step [100/938], Loss: 16032.2754\n",
            "VAE train Epoch [35/50], Step [200/938], Loss: 15319.4248\n",
            "VAE train Epoch [35/50], Step [300/938], Loss: 15693.5498\n",
            "VAE train Epoch [35/50], Step [400/938], Loss: 15980.5518\n",
            "VAE train Epoch [35/50], Step [500/938], Loss: 14813.2695\n",
            "VAE train Epoch [35/50], Step [600/938], Loss: 15414.9551\n",
            "VAE train Epoch [35/50], Step [700/938], Loss: 15315.3574\n",
            "VAE train Epoch [35/50], Step [800/938], Loss: 15937.8418\n",
            "VAE train Epoch [35/50], Step [900/938], Loss: 15238.9365\n",
            "VAE train Epoch [36/50], Step [100/938], Loss: 15513.0273\n",
            "VAE train Epoch [36/50], Step [200/938], Loss: 14039.0684\n",
            "VAE train Epoch [36/50], Step [300/938], Loss: 15842.3311\n",
            "VAE train Epoch [36/50], Step [400/938], Loss: 14980.8291\n",
            "VAE train Epoch [36/50], Step [500/938], Loss: 14392.2578\n",
            "VAE train Epoch [36/50], Step [600/938], Loss: 15397.8027\n",
            "VAE train Epoch [36/50], Step [700/938], Loss: 14964.4766\n",
            "VAE train Epoch [36/50], Step [800/938], Loss: 15373.6387\n",
            "VAE train Epoch [36/50], Step [900/938], Loss: 15606.1270\n",
            "VAE train Epoch [37/50], Step [100/938], Loss: 14940.5117\n",
            "VAE train Epoch [37/50], Step [200/938], Loss: 15503.5566\n",
            "VAE train Epoch [37/50], Step [300/938], Loss: 14501.3369\n",
            "VAE train Epoch [37/50], Step [400/938], Loss: 14894.3301\n",
            "VAE train Epoch [37/50], Step [500/938], Loss: 15746.4180\n",
            "VAE train Epoch [37/50], Step [600/938], Loss: 14230.1836\n",
            "VAE train Epoch [37/50], Step [700/938], Loss: 16220.0586\n",
            "VAE train Epoch [37/50], Step [800/938], Loss: 15409.9980\n",
            "VAE train Epoch [37/50], Step [900/938], Loss: 14863.5869\n",
            "VAE train Epoch [38/50], Step [100/938], Loss: 16162.4336\n",
            "VAE train Epoch [38/50], Step [200/938], Loss: 15576.6328\n",
            "VAE train Epoch [38/50], Step [300/938], Loss: 15224.3252\n",
            "VAE train Epoch [38/50], Step [400/938], Loss: 15254.5039\n",
            "VAE train Epoch [38/50], Step [500/938], Loss: 15426.0820\n",
            "VAE train Epoch [38/50], Step [600/938], Loss: 15849.6504\n",
            "VAE train Epoch [38/50], Step [700/938], Loss: 15326.1611\n",
            "VAE train Epoch [38/50], Step [800/938], Loss: 15863.9639\n",
            "VAE train Epoch [38/50], Step [900/938], Loss: 15694.0479\n",
            "VAE train Epoch [39/50], Step [100/938], Loss: 15335.1670\n",
            "VAE train Epoch [39/50], Step [200/938], Loss: 14502.1885\n",
            "VAE train Epoch [39/50], Step [300/938], Loss: 14844.3984\n",
            "VAE train Epoch [39/50], Step [400/938], Loss: 14489.8955\n",
            "VAE train Epoch [39/50], Step [500/938], Loss: 14961.3711\n",
            "VAE train Epoch [39/50], Step [600/938], Loss: 16300.3418\n",
            "VAE train Epoch [39/50], Step [700/938], Loss: 15200.0547\n",
            "VAE train Epoch [39/50], Step [800/938], Loss: 15653.8350\n",
            "VAE train Epoch [39/50], Step [900/938], Loss: 16201.5752\n",
            "VAE train Epoch [40/50], Step [100/938], Loss: 15493.6328\n",
            "VAE train Epoch [40/50], Step [200/938], Loss: 15708.0771\n",
            "VAE train Epoch [40/50], Step [300/938], Loss: 15039.4004\n",
            "VAE train Epoch [40/50], Step [400/938], Loss: 15619.2412\n",
            "VAE train Epoch [40/50], Step [500/938], Loss: 15863.2207\n",
            "VAE train Epoch [40/50], Step [600/938], Loss: 16098.7441\n",
            "VAE train Epoch [40/50], Step [700/938], Loss: 15417.4785\n",
            "VAE train Epoch [40/50], Step [800/938], Loss: 14156.3340\n",
            "VAE train Epoch [40/50], Step [900/938], Loss: 14594.6943\n",
            "VAE train Epoch [41/50], Step [100/938], Loss: 14824.8379\n",
            "VAE train Epoch [41/50], Step [200/938], Loss: 15059.6543\n",
            "VAE train Epoch [41/50], Step [300/938], Loss: 14901.1338\n",
            "VAE train Epoch [41/50], Step [400/938], Loss: 15435.3213\n",
            "VAE train Epoch [41/50], Step [500/938], Loss: 15483.3584\n",
            "VAE train Epoch [41/50], Step [600/938], Loss: 15177.2871\n",
            "VAE train Epoch [41/50], Step [700/938], Loss: 16500.5371\n",
            "VAE train Epoch [41/50], Step [800/938], Loss: 15478.6377\n",
            "VAE train Epoch [41/50], Step [900/938], Loss: 15100.7617\n",
            "VAE train Epoch [42/50], Step [100/938], Loss: 17482.3008\n",
            "VAE train Epoch [42/50], Step [200/938], Loss: 15049.4453\n",
            "VAE train Epoch [42/50], Step [300/938], Loss: 14786.9805\n",
            "VAE train Epoch [42/50], Step [400/938], Loss: 14543.0254\n",
            "VAE train Epoch [42/50], Step [500/938], Loss: 14498.1240\n",
            "VAE train Epoch [42/50], Step [600/938], Loss: 15137.2891\n",
            "VAE train Epoch [42/50], Step [700/938], Loss: 15549.5557\n",
            "VAE train Epoch [42/50], Step [800/938], Loss: 15420.6602\n",
            "VAE train Epoch [42/50], Step [900/938], Loss: 15060.6455\n",
            "VAE train Epoch [43/50], Step [100/938], Loss: 14578.9072\n",
            "VAE train Epoch [43/50], Step [200/938], Loss: 14726.5469\n",
            "VAE train Epoch [43/50], Step [300/938], Loss: 15507.6934\n",
            "VAE train Epoch [43/50], Step [400/938], Loss: 14713.4736\n",
            "VAE train Epoch [43/50], Step [500/938], Loss: 15401.3252\n",
            "VAE train Epoch [43/50], Step [600/938], Loss: 14536.1738\n",
            "VAE train Epoch [43/50], Step [700/938], Loss: 15506.0010\n",
            "VAE train Epoch [43/50], Step [800/938], Loss: 15493.4941\n",
            "VAE train Epoch [43/50], Step [900/938], Loss: 16102.4561\n",
            "VAE train Epoch [44/50], Step [100/938], Loss: 15578.3994\n",
            "VAE train Epoch [44/50], Step [200/938], Loss: 15002.5742\n",
            "VAE train Epoch [44/50], Step [300/938], Loss: 14277.8203\n",
            "VAE train Epoch [44/50], Step [400/938], Loss: 14849.7656\n",
            "VAE train Epoch [44/50], Step [500/938], Loss: 15666.3115\n",
            "VAE train Epoch [44/50], Step [600/938], Loss: 14893.1104\n",
            "VAE train Epoch [44/50], Step [700/938], Loss: 15735.2705\n",
            "VAE train Epoch [44/50], Step [800/938], Loss: 15657.1895\n",
            "VAE train Epoch [44/50], Step [900/938], Loss: 15248.2656\n",
            "VAE train Epoch [45/50], Step [100/938], Loss: 15566.4570\n",
            "VAE train Epoch [45/50], Step [200/938], Loss: 15010.8164\n",
            "VAE train Epoch [45/50], Step [300/938], Loss: 15464.6553\n",
            "VAE train Epoch [45/50], Step [400/938], Loss: 16166.6113\n",
            "VAE train Epoch [45/50], Step [500/938], Loss: 15230.2051\n",
            "VAE train Epoch [45/50], Step [600/938], Loss: 15609.4043\n",
            "VAE train Epoch [45/50], Step [700/938], Loss: 15842.5098\n",
            "VAE train Epoch [45/50], Step [800/938], Loss: 15785.3672\n",
            "VAE train Epoch [45/50], Step [900/938], Loss: 15510.1934\n",
            "VAE train Epoch [46/50], Step [100/938], Loss: 14705.9805\n",
            "VAE train Epoch [46/50], Step [200/938], Loss: 15405.9336\n",
            "VAE train Epoch [46/50], Step [300/938], Loss: 14497.0312\n",
            "VAE train Epoch [46/50], Step [400/938], Loss: 15125.8496\n",
            "VAE train Epoch [46/50], Step [500/938], Loss: 15700.3223\n",
            "VAE train Epoch [46/50], Step [600/938], Loss: 15030.5889\n",
            "VAE train Epoch [46/50], Step [700/938], Loss: 15625.0908\n",
            "VAE train Epoch [46/50], Step [800/938], Loss: 15370.5723\n",
            "VAE train Epoch [46/50], Step [900/938], Loss: 15321.8730\n",
            "VAE train Epoch [47/50], Step [100/938], Loss: 15195.6973\n",
            "VAE train Epoch [47/50], Step [200/938], Loss: 15503.4629\n",
            "VAE train Epoch [47/50], Step [300/938], Loss: 15385.4033\n",
            "VAE train Epoch [47/50], Step [400/938], Loss: 15272.8320\n",
            "VAE train Epoch [47/50], Step [500/938], Loss: 15653.2793\n",
            "VAE train Epoch [47/50], Step [600/938], Loss: 16645.6270\n",
            "VAE train Epoch [47/50], Step [700/938], Loss: 14952.1768\n",
            "VAE train Epoch [47/50], Step [800/938], Loss: 15519.3945\n",
            "VAE train Epoch [47/50], Step [900/938], Loss: 15305.8730\n",
            "VAE train Epoch [48/50], Step [100/938], Loss: 14014.9961\n",
            "VAE train Epoch [48/50], Step [200/938], Loss: 15021.1260\n",
            "VAE train Epoch [48/50], Step [300/938], Loss: 15101.1943\n",
            "VAE train Epoch [48/50], Step [400/938], Loss: 14666.6748\n",
            "VAE train Epoch [48/50], Step [500/938], Loss: 15047.9912\n",
            "VAE train Epoch [48/50], Step [600/938], Loss: 14941.2471\n",
            "VAE train Epoch [48/50], Step [700/938], Loss: 15401.9346\n",
            "VAE train Epoch [48/50], Step [800/938], Loss: 15764.5186\n",
            "VAE train Epoch [48/50], Step [900/938], Loss: 15871.8418\n",
            "VAE train Epoch [49/50], Step [100/938], Loss: 15766.1963\n",
            "VAE train Epoch [49/50], Step [200/938], Loss: 15480.4189\n",
            "VAE train Epoch [49/50], Step [300/938], Loss: 16184.2334\n",
            "VAE train Epoch [49/50], Step [400/938], Loss: 17565.2109\n",
            "VAE train Epoch [49/50], Step [500/938], Loss: 14445.0918\n",
            "VAE train Epoch [49/50], Step [600/938], Loss: 16090.0137\n",
            "VAE train Epoch [49/50], Step [700/938], Loss: 14680.9199\n",
            "VAE train Epoch [49/50], Step [800/938], Loss: 15578.5791\n",
            "VAE train Epoch [49/50], Step [900/938], Loss: 15434.0400\n",
            "VAE train Epoch [50/50], Step [100/938], Loss: 14909.0820\n",
            "VAE train Epoch [50/50], Step [200/938], Loss: 15336.4434\n",
            "VAE train Epoch [50/50], Step [300/938], Loss: 14277.2139\n",
            "VAE train Epoch [50/50], Step [400/938], Loss: 14763.7793\n",
            "VAE train Epoch [50/50], Step [500/938], Loss: 13990.0684\n",
            "VAE train Epoch [50/50], Step [600/938], Loss: 15790.5195\n",
            "VAE train Epoch [50/50], Step [700/938], Loss: 16156.3945\n",
            "VAE train Epoch [50/50], Step [800/938], Loss: 14665.4502\n",
            "VAE train Epoch [50/50], Step [900/938], Loss: 14938.0664\n",
            "model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_SVN(num_of_labeled_samples, expansion_rate, VAE_model):\n",
        "  # Split the train dataset\n",
        "  print(f'Spliting the training dataset to {num_of_labeled_samples} labled samples')\n",
        "  labled_dataset = split_to_labeled(train_dataset, num_of_labeled_samples)\n",
        "\n",
        "  z_data = torch.zeros([0,50]).to(device)\n",
        "  z_label = torch.zeros([0]).to(device)\n",
        "\n",
        "\n",
        "\n",
        "  #create the expanded latent vector space\n",
        "  print('Learning mu and logvar from labeled dataset')\n",
        "  data_loader = DataLoader(labled_dataset, batch_size=VAE_batch_size, shuffle=False)\n",
        "  for batch in data_loader:\n",
        "      images, _ = batch  # Assuming you don't need the labels\n",
        "      print(f'batch.shape={images.size()}')\n",
        "      images = images.view(images.size(0), -1).to(device)\n",
        "      _, mu, logvar = VAE_model(images)\n",
        "\n",
        "      print(f'Expanding the labeled dataset by {expansion_rate}')\n",
        "      for i in range(0,len(mu)):\n",
        "        for e in range(0,expansion_rate):\n",
        "          new_z = reparameterize(mu[i], logvar[i])\n",
        "          new_z = (new_z.unsqueeze(0))\n",
        "          new_label = torch.tensor(labled_dataset.labels[i]).to(device)\n",
        "          new_label = (new_label.unsqueeze(0))\n",
        "          z_data = torch.cat((z_data, new_z), 0)\n",
        "          z_label = torch.cat((z_label, new_label), 0).to(torch.int)\n",
        "\n",
        "  print(f'after z_data.shape = {z_data.shape}')\n",
        "  print(f'after z_label.shape = {z_label.shape}')\n",
        "\n",
        "  print(f'Total entries to SVM train: {z_data.size()}')\n",
        "\n",
        "  SVM = SVM_model(kernel='poly', degree=3)\n",
        "\n",
        "  print('Training the SVM with expanded latent dataset')\n",
        "  latent_dataset = Dataset()\n",
        "  latent_dataset.data = z_data\n",
        "  latent_dataset.targets = z_label\n",
        "\n",
        "  SVM = train_SVM(SVM, latent_dataset)\n",
        "\n",
        "  return SVM\n"
      ],
      "metadata": {
        "id": "ikcreYJa-n3_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "test_z_data = torch.zeros([0,50]).to(device)\n",
        "test_z_label = torch.zeros([0]).to(device)\n",
        "\n",
        "i = 0\n",
        "for batch in data_loader:\n",
        "    images, _ = batch  # Assuming you don't need the labels\n",
        "    images = images.view(images.size(0), -1).to(device)\n",
        "    _, mu, logvar = VAE_model(images)\n",
        "    new_z = reparameterize(mu, logvar)\n",
        "    new_z = (new_z.squeeze(1))\n",
        "    new_label = test_dataset.targets[i].clone().detach().to(device)\n",
        "    new_label1 = new_label.unsqueeze(0)\n",
        "    test_z_data = torch.cat((test_z_data, new_z), 0)\n",
        "    test_z_label = torch.cat((test_z_label, new_label1), 0).to(torch.int)\n",
        "    i += 1\n",
        "\n",
        "test_latent_dataset = Dataset()\n",
        "test_latent_dataset.data = test_z_data\n",
        "test_latent_dataset.targets = test_z_label\n",
        "\n",
        "for num_of_labeled_samples in [100, 600, 1000, 3000]:\n",
        "  for expansion_rate in [1, 10, 20]:\n",
        "    SVM = create_SVN(num_of_labeled_samples, expansion_rate, VAE_model)\n",
        "    result = test_SVM(SVM, test_latent_dataset)\n",
        "\n",
        "    print(f'Final result of SVM on test dataset: {result} with number of labeled samples: {num_of_labeled_samples} and expansion rate of: {expansion_rate}')\n"
      ],
      "metadata": {
        "id": "c0jAuPefSoTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eade09d5-7588-49ad-d318-b165b003c5c5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spliting the training dataset to 100 labled samples\n",
            "Learning mu and logvar from labeled dataset\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([36, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "after z_data.shape = torch.Size([100, 50])\n",
            "after z_label.shape = torch.Size([100])\n",
            "Total entries to SVM train: torch.Size([100, 50])\n",
            "Training the SVM with expanded latent dataset\n",
            "Final result of SVM on test dataset: 0.1008 with number of labeled samples: 100 and expansion rate of: 1\n",
            "Spliting the training dataset to 100 labled samples\n",
            "Learning mu and logvar from labeled dataset\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([36, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "after z_data.shape = torch.Size([1000, 50])\n",
            "after z_label.shape = torch.Size([1000])\n",
            "Total entries to SVM train: torch.Size([1000, 50])\n",
            "Training the SVM with expanded latent dataset\n",
            "Final result of SVM on test dataset: 0.2084 with number of labeled samples: 100 and expansion rate of: 10\n",
            "Spliting the training dataset to 100 labled samples\n",
            "Learning mu and logvar from labeled dataset\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([36, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "after z_data.shape = torch.Size([2000, 50])\n",
            "after z_label.shape = torch.Size([2000])\n",
            "Total entries to SVM train: torch.Size([2000, 50])\n",
            "Training the SVM with expanded latent dataset\n",
            "Final result of SVM on test dataset: 0.3123 with number of labeled samples: 100 and expansion rate of: 20\n",
            "Spliting the training dataset to 600 labled samples\n",
            "Learning mu and logvar from labeled dataset\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([24, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "after z_data.shape = torch.Size([600, 50])\n",
            "after z_label.shape = torch.Size([600])\n",
            "Total entries to SVM train: torch.Size([600, 50])\n",
            "Training the SVM with expanded latent dataset\n",
            "Final result of SVM on test dataset: 0.1209 with number of labeled samples: 600 and expansion rate of: 1\n",
            "Spliting the training dataset to 600 labled samples\n",
            "Learning mu and logvar from labeled dataset\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([24, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "after z_data.shape = torch.Size([6000, 50])\n",
            "after z_label.shape = torch.Size([6000])\n",
            "Total entries to SVM train: torch.Size([6000, 50])\n",
            "Training the SVM with expanded latent dataset\n",
            "Final result of SVM on test dataset: 0.1513 with number of labeled samples: 600 and expansion rate of: 10\n",
            "Spliting the training dataset to 600 labled samples\n",
            "Learning mu and logvar from labeled dataset\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([24, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "after z_data.shape = torch.Size([12000, 50])\n",
            "after z_label.shape = torch.Size([12000])\n",
            "Total entries to SVM train: torch.Size([12000, 50])\n",
            "Training the SVM with expanded latent dataset\n",
            "Final result of SVM on test dataset: 0.1526 with number of labeled samples: 600 and expansion rate of: 20\n",
            "Spliting the training dataset to 1000 labled samples\n",
            "Learning mu and logvar from labeled dataset\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([40, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "after z_data.shape = torch.Size([1000, 50])\n",
            "after z_label.shape = torch.Size([1000])\n",
            "Total entries to SVM train: torch.Size([1000, 50])\n",
            "Training the SVM with expanded latent dataset\n",
            "Final result of SVM on test dataset: 0.1145 with number of labeled samples: 1000 and expansion rate of: 1\n",
            "Spliting the training dataset to 1000 labled samples\n",
            "Learning mu and logvar from labeled dataset\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([40, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "after z_data.shape = torch.Size([10000, 50])\n",
            "after z_label.shape = torch.Size([10000])\n",
            "Total entries to SVM train: torch.Size([10000, 50])\n",
            "Training the SVM with expanded latent dataset\n",
            "Final result of SVM on test dataset: 0.1295 with number of labeled samples: 1000 and expansion rate of: 10\n",
            "Spliting the training dataset to 1000 labled samples\n",
            "Learning mu and logvar from labeled dataset\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([40, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "after z_data.shape = torch.Size([20000, 50])\n",
            "after z_label.shape = torch.Size([20000])\n",
            "Total entries to SVM train: torch.Size([20000, 50])\n",
            "Training the SVM with expanded latent dataset\n",
            "Final result of SVM on test dataset: 0.1406 with number of labeled samples: 1000 and expansion rate of: 20\n",
            "Spliting the training dataset to 3000 labled samples\n",
            "Learning mu and logvar from labeled dataset\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "batch.shape=torch.Size([56, 1, 28, 28])\n",
            "Expanding the labeled dataset by 1\n",
            "after z_data.shape = torch.Size([3000, 50])\n",
            "after z_label.shape = torch.Size([3000])\n",
            "Total entries to SVM train: torch.Size([3000, 50])\n",
            "Training the SVM with expanded latent dataset\n",
            "Final result of SVM on test dataset: 0.1116 with number of labeled samples: 3000 and expansion rate of: 1\n",
            "Spliting the training dataset to 3000 labled samples\n",
            "Learning mu and logvar from labeled dataset\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "batch.shape=torch.Size([56, 1, 28, 28])\n",
            "Expanding the labeled dataset by 10\n",
            "after z_data.shape = torch.Size([30000, 50])\n",
            "after z_label.shape = torch.Size([30000])\n",
            "Total entries to SVM train: torch.Size([30000, 50])\n",
            "Training the SVM with expanded latent dataset\n",
            "Final result of SVM on test dataset: 0.1149 with number of labeled samples: 3000 and expansion rate of: 10\n",
            "Spliting the training dataset to 3000 labled samples\n",
            "Learning mu and logvar from labeled dataset\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([64, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "batch.shape=torch.Size([56, 1, 28, 28])\n",
            "Expanding the labeled dataset by 20\n",
            "after z_data.shape = torch.Size([60000, 50])\n",
            "after z_label.shape = torch.Size([60000])\n",
            "Total entries to SVM train: torch.Size([60000, 50])\n",
            "Training the SVM with expanded latent dataset\n",
            "Final result of SVM on test dataset: 0.1118 with number of labeled samples: 3000 and expansion rate of: 20\n"
          ]
        }
      ]
    }
  ]
}